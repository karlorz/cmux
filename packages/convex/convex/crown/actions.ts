"use node";

import { createAnthropic } from "@ai-sdk/anthropic";
import { createGoogleGenerativeAI } from "@ai-sdk/google";
import { createOpenAI } from "@ai-sdk/openai";
import { generateObject, type LanguageModel } from "ai";
import { ConvexError, v } from "convex/values";
import {
  CrownEvaluationResponseSchema,
  CrownSummarizationResponseSchema,
  type CrownEvaluationCandidate,
  type CrownEvaluationResponse,
  type CrownSummarizationResponse,
} from "@cmux/shared/convex-safe";
import {
  CLOUDFLARE_OPENAI_BASE_URL,
  CLOUDFLARE_ANTHROPIC_BASE_URL,
  CLOUDFLARE_GEMINI_BASE_URL,
} from "@cmux/shared";
import { action, internalAction } from "../_generated/server";
import type { ActionCtx } from "../_generated/server";
import { internal } from "../_generated/api";
import type { Id } from "../_generated/dataModel";
import { parseCrownEvaluationPrompt } from "./retryData";

const OPENAI_CROWN_MODEL = "gpt-5-mini-2025-08-07";
const ANTHROPIC_CROWN_MODEL = "claude-sonnet-4-5-20250929";
const GEMINI_CROWN_MODEL = "gemini-3-flash-preview";

const CROWN_PROVIDERS = ["gemini", "openai", "anthropic"] as const;
type CrownProvider = (typeof CROWN_PROVIDERS)[number];

// Configuration for retry logic
const MAX_CROWN_EVALUATION_ATTEMPTS = 3;
const MAX_CROWN_SUMMARIZATION_ATTEMPTS = 3;
const RETRY_BASE_DELAY_MS = 1000; // 1 second base delay, doubles each retry

/**
 * Build PR title from task prompt (inline version for Convex action)
 */
function buildPullRequestTitle(prompt: string): string {
  const base = prompt.trim() || "cmux changes";
  const title = `[Crown] ${base}`;
  return title.length > 72 ? `${title.slice(0, 69)}...` : title;
}

/**
 * Build PR body/description from task details (inline version for Convex action)
 */
function buildPullRequestBody({
  summary,
  prompt,
  agentName,
  branch,
  taskId,
  runId,
}: {
  summary?: string;
  prompt: string;
  agentName: string;
  branch: string;
  taskId: string;
  runId: string;
}): string {
  const bodySummary = summary?.trim() || "Summary not available.";
  return `## Crown Winner: ${agentName}

### Task Description
${prompt}

### Summary
${bodySummary}

### Implementation Details
- **Agent**: ${agentName}
- **Task ID**: ${taskId}
- **Run ID**: ${runId}
- **Branch**: ${branch}
- **Created**: ${new Date().toISOString()}`;
}

/**
 * Delays execution for exponential backoff
 */
function delay(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

const CrownEvaluationCandidateValidator = v.object({
  runId: v.optional(v.string()),
  agentName: v.optional(v.string()),
  modelName: v.optional(v.string()),
  gitDiff: v.string(),
  newBranch: v.optional(v.union(v.string(), v.null())),
  index: v.optional(v.number()),
});

function resolveCrownModel(): {
  provider: CrownProvider;
  model: LanguageModel;
} {
  // Note: AIGATEWAY_* accessed via process.env to avoid Convex static analysis
  const geminiKey = process.env.GEMINI_API_KEY;
  if (geminiKey) {
    const google = createGoogleGenerativeAI({
      apiKey: geminiKey,
      baseURL: process.env.AIGATEWAY_GEMINI_BASE_URL || CLOUDFLARE_GEMINI_BASE_URL,
    });
    return { provider: "gemini", model: google(GEMINI_CROWN_MODEL) };
  }

  const openaiKey = process.env.OPENAI_API_KEY;
  if (openaiKey) {
    const openai = createOpenAI({
      apiKey: openaiKey,
      baseURL: process.env.AIGATEWAY_OPENAI_BASE_URL || CLOUDFLARE_OPENAI_BASE_URL,
    });
    return { provider: "openai", model: openai(OPENAI_CROWN_MODEL) };
  }

  const anthropicKey = process.env.ANTHROPIC_API_KEY;
  if (anthropicKey) {
    const anthropic = createAnthropic({
      apiKey: anthropicKey,
      baseURL:
        process.env.AIGATEWAY_ANTHROPIC_BASE_URL || CLOUDFLARE_ANTHROPIC_BASE_URL,
    });
    return {
      provider: "anthropic",
      model: anthropic(ANTHROPIC_CROWN_MODEL),
    };
  }

  throw new ConvexError(
    "Crown evaluation is not configured (missing Gemini, OpenAI, or Anthropic API key)"
  );
}

export async function performCrownEvaluation(
  prompt: string,
  candidates: CrownEvaluationCandidate[]
): Promise<CrownEvaluationResponse> {
  const { model, provider } = resolveCrownModel();

  const normalizedCandidates = candidates.map((candidate, idx) => {
    const resolvedIndex = candidate.index ?? idx;
    return {
      index: resolvedIndex,
      runId: candidate.runId,
      agentName: candidate.agentName,
      modelName:
        candidate.modelName ??
        candidate.agentName ??
        (candidate.runId ? `run-${candidate.runId}` : undefined) ??
        `candidate-${resolvedIndex}`,
      gitDiff: candidate.gitDiff,
      newBranch: candidate.newBranch ?? null,
    };
  });

  const evaluationData = {
    prompt,
    candidates: normalizedCandidates,
  };

  const evaluationPrompt = `You are evaluating code implementations from different AI models.

Here are the candidates to evaluate:
${JSON.stringify(evaluationData, null, 2)}

NOTE: The git diffs shown contain only actual code changes. Lock files, build artifacts, and other non-essential files have been filtered out.

Analyze these implementations and select the best one based on:
1. Code quality and correctness
2. Completeness of the solution
3. Following best practices
4. Actually having meaningful code changes (if one has no changes, prefer the one with changes)

Respond with a JSON object containing:
- "winner": the index (0-based) of the best implementation
- "reason": a brief explanation of why this implementation was chosen

Example response:
{"winner": 0, "reason": "Model claude/sonnet-4 provided a more complete implementation with better error handling and cleaner code structure."}

IMPORTANT: Respond ONLY with the JSON object, no other text.`;

  // Track errors for diagnostics
  const attemptErrors: Array<{ attempt: number; error: unknown }> = [];

  // Retry loop with exponential backoff
  for (let attempt = 1; attempt <= MAX_CROWN_EVALUATION_ATTEMPTS; attempt++) {
    try {
      console.info(
        `[convex.crown] Evaluation attempt ${attempt}/${MAX_CROWN_EVALUATION_ATTEMPTS} via ${provider}`
      );

      const { object } = await generateObject({
        model,
        schema: CrownEvaluationResponseSchema,
        system:
          "You select the best implementation from structured diff inputs and explain briefly why.",
        prompt: evaluationPrompt,
        maxRetries: 2,
      });

      console.info(`[convex.crown] Evaluation completed via ${provider}`);
      const result = CrownEvaluationResponseSchema.parse(object);

      // If AI returned null winner (e.g., no code changes), default to candidate 0
      // This is different from isFallback=true which indicates AI service failure
      if (result.winner === null && normalizedCandidates.length > 0) {
        console.info(
          `[convex.crown] AI returned null winner, defaulting to candidate 0`
        );
        return {
          ...result,
          winner: 0,
          reason:
            result.reason ||
            "No meaningful code changes detected; selecting first candidate as default.",
        };
      }

      return result;
    } catch (error) {
      attemptErrors.push({ attempt, error });

      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error(
        `[convex.crown] ${provider} evaluation attempt ${attempt}/${MAX_CROWN_EVALUATION_ATTEMPTS} failed:`,
        errorMessage
      );

      // If not the last attempt, wait with exponential backoff before retrying
      if (attempt < MAX_CROWN_EVALUATION_ATTEMPTS) {
        const backoffDelay = RETRY_BASE_DELAY_MS * Math.pow(2, attempt - 1);
        console.info(
          `[convex.crown] Retrying in ${backoffDelay}ms (attempt ${attempt + 1}/${MAX_CROWN_EVALUATION_ATTEMPTS})`
        );
        await delay(backoffDelay);
      }
    }
  }

  // All retry attempts exhausted - fall back to "no winner" state
  console.warn(
    `[convex.crown] All ${MAX_CROWN_EVALUATION_ATTEMPTS} evaluation attempts failed via ${provider}. Falling back to no-winner state.`,
    {
      provider,
      totalAttempts: MAX_CROWN_EVALUATION_ATTEMPTS,
      errors: attemptErrors.map((e) => ({
        attempt: e.attempt,
        error: e.error instanceof Error ? e.error.message : String(e.error),
      })),
    }
  );

  // Return fallback response with metadata (null winner indicates failure)
  const fallbackReason =
    "Evaluation service unavailable - no winner selected";
  return {
    winner: null,
    reason: fallbackReason,
    isFallback: true,
    evaluationNote: `Crown evaluation failed after ${MAX_CROWN_EVALUATION_ATTEMPTS} attempts (provider: ${provider}). No winner was selected.`,
  };
}

export async function performCrownSummarization(
  prompt: string,
  gitDiff: string
): Promise<CrownSummarizationResponse> {
  const { model, provider } = resolveCrownModel();

  const summarizationPrompt = `You are an expert reviewer summarizing a pull request.

GOAL
- Explain succinctly what changed and why.
- Call out areas the user should review carefully.
- Provide a quick test plan to validate the changes.

CONTEXT
- User's original request:
${prompt}
- Relevant diffs (unified):
${gitDiff || "<no code changes captured>"}

INSTRUCTIONS
- Base your summary strictly on the provided diffs and request.
- Be specific about files and functions when possible.
- Prefer clear bullet points over prose. Keep it under ~300 words.
- If there are no code changes, say so explicitly and suggest next steps.

OUTPUT FORMAT (Markdown)
## PR Review Summary
- What Changed: bullet list
- Review Focus: bullet list (risks/edge cases)
- Test Plan: bullet list of practical steps
- Follow-ups: optional bullets if applicable
`;

  const attemptErrors: Array<{ attempt: number; error: unknown }> = [];

  for (let attempt = 1; attempt <= MAX_CROWN_SUMMARIZATION_ATTEMPTS; attempt++) {
    try {
      console.info(
        `[convex.crown] Summarization attempt ${attempt}/${MAX_CROWN_SUMMARIZATION_ATTEMPTS} via ${provider}`
      );

      const { object } = await generateObject({
        model,
        schema: CrownSummarizationResponseSchema,
        system:
          "You are an expert reviewer summarizing pull requests. Provide a clear, concise summary following the requested format.",
        prompt: summarizationPrompt,
        maxRetries: 2,
      });

      console.info(`[convex.crown] Summarization completed via ${provider}`);
      return CrownSummarizationResponseSchema.parse(object);
    } catch (error) {
      attemptErrors.push({ attempt, error });
      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error(
        `[convex.crown] ${provider} summarization attempt ${attempt}/${MAX_CROWN_SUMMARIZATION_ATTEMPTS} failed:`,
        errorMessage
      );

      if (attempt < MAX_CROWN_SUMMARIZATION_ATTEMPTS) {
        const backoffDelay = RETRY_BASE_DELAY_MS * Math.pow(2, attempt - 1);
        console.info(
          `[convex.crown] Retrying summarization in ${backoffDelay}ms (attempt ${attempt + 1}/${MAX_CROWN_SUMMARIZATION_ATTEMPTS})`
        );
        await delay(backoffDelay);
      }
    }
  }

  const lastError = attemptErrors[attemptErrors.length - 1]?.error;
  const lastMessage =
    lastError instanceof Error ? lastError.message : String(lastError ?? "");
  console.warn(
    `[convex.crown] All ${MAX_CROWN_SUMMARIZATION_ATTEMPTS} summarization attempts failed via ${provider}`,
    {
      provider,
      totalAttempts: MAX_CROWN_SUMMARIZATION_ATTEMPTS,
      lastMessage,
    }
  );

  throw new ConvexError(
    `Summarization failed after ${MAX_CROWN_SUMMARIZATION_ATTEMPTS} attempts (provider: ${provider})`
  );
}

export const evaluate = action({
  args: {
    prompt: v.string(),
    candidates: v.array(CrownEvaluationCandidateValidator),
    teamSlugOrId: v.string(),
  },
  handler: async (_ctx, args) => {
    return performCrownEvaluation(args.prompt, args.candidates);
  },
});

export const summarize = action({
  args: {
    prompt: v.string(),
    gitDiff: v.string(),
    teamSlugOrId: v.string(),
  },
  handler: async (_ctx, args) => {
    return performCrownSummarization(args.prompt, args.gitDiff);
  },
});

/**
 * Schema for parsing the stored retry data
 */
interface CrownRetryData {
  evaluationPrompt: string;
  candidateRunIds: string[];
  teamId: string;
  userId: string;
}

/**
 * Schema for parsing evaluation prompt to extract candidates
 * Format: "Task: <prompt>\nCandidates: <JSON array>"
 */
/**
 * Internal action to retry a failed crown evaluation.
 * Called after retryCrownEvaluation mutation resets the status.
 */
export const retryEvaluation = internalAction({
  args: {
    taskId: v.id("tasks"),
  },
  handler: async (ctx, args) => {
    console.log(`[Crown] Starting retry evaluation for task ${args.taskId}`);

    // Get task with retry data
    const task = await ctx.runQuery(internal.tasks.getByIdInternal, {
      id: args.taskId,
    });

    if (!task) {
      throw new Error("Task not found");
    }

    if (!task.crownEvaluationRetryData) {
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: task.teamId,
        userId: task.userId,
        status: "error",
        errorMessage: "No retry data available for this task",
      });
      throw new Error("No retry data available for this task");
    }

    // Parse retry data
    let retryData: CrownRetryData;
    try {
      retryData = JSON.parse(task.crownEvaluationRetryData);
    } catch (error) {
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: task.teamId,
        userId: task.userId,
        status: "error",
        errorMessage: "Invalid retry data format",
      });
      throw new Error("Invalid retry data format");
    }

    // Parse the evaluation prompt to extract candidates
    const parsedData = parseCrownEvaluationPrompt(retryData.evaluationPrompt);
    if (!parsedData) {
      // Mark as error if we can't parse the data
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: retryData.teamId,
        userId: retryData.userId,
        status: "error",
        errorMessage: "Failed to parse stored evaluation data",
      });
      throw new Error("Failed to parse stored evaluation data");
    }

    // Mark as in_progress
    await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
      taskId: args.taskId,
      teamId: retryData.teamId,
      userId: retryData.userId,
      status: "in_progress",
      clearError: true,
    });

    // Prepare candidates for evaluation
    const evaluationCandidates: CrownEvaluationCandidate[] = parsedData.candidates.map(
      (candidate, idx) => ({
        runId: candidate.runId,
        agentName: candidate.agentName,
        modelName: candidate.modelName || candidate.agentName,
        gitDiff: candidate.gitDiff,
        newBranch: candidate.newBranch,
        index: candidate.index ?? idx,
      })
    );

    console.log(`[Crown] Retrying evaluation with ${evaluationCandidates.length} candidates`);

    try {
      // Perform the evaluation
      const evaluationResponse = await performCrownEvaluation(
        parsedData.prompt,
        evaluationCandidates
      );

      console.log(`[Crown] Retry evaluation result:`, {
        winner: evaluationResponse.winner,
        isFallback: evaluationResponse.isFallback,
      });

      // Handle the result
      if (evaluationResponse.winner === null) {
        // Still failing - update error state
        await ctx.runMutation(internal.crown.workerFinalize, {
          taskId: args.taskId,
          teamId: retryData.teamId,
          userId: retryData.userId,
          winnerRunId: null,
          reason: evaluationResponse.reason,
          evaluationPrompt: retryData.evaluationPrompt,
          evaluationResponse: JSON.stringify(evaluationResponse),
          candidateRunIds: retryData.candidateRunIds.map((id) => id as Id<"taskRuns">),
          isFallback: true,
          evaluationNote: evaluationResponse.evaluationNote || "Retry evaluation failed",
        });

        return { success: false, reason: evaluationResponse.reason };
      }

      // Success! Get the winner candidate
      const winnerCandidate = parsedData.candidates[evaluationResponse.winner];
      if (!winnerCandidate) {
        throw new Error(`Winner index ${evaluationResponse.winner} out of bounds`);
      }

      // Generate summary for the winning candidate (atomic: must succeed)
      let summary: string | undefined;
      try {
        console.log(
          `[Crown] Generating summary for retry winner ${winnerCandidate.runId}`
        );
        const summaryResponse = await performCrownSummarization(
          parsedData.prompt,
          winnerCandidate.gitDiff
        );
        summary = summaryResponse?.summary?.slice(0, 8000);
      } catch (summaryError) {
        const message =
          summaryError instanceof Error
            ? summaryError.message
            : String(summaryError);

        console.error(
          "[Crown] Summary generation failed during retry; marking crown as failed",
          { taskId: args.taskId, error: message }
        );

        await ctx.runMutation(internal.crown.workerFinalize, {
          taskId: args.taskId,
          teamId: retryData.teamId,
          userId: retryData.userId,
          winnerRunId: null,
          reason: "Summarization failed",
          evaluationPrompt: retryData.evaluationPrompt,
          evaluationResponse: JSON.stringify(evaluationResponse),
          candidateRunIds: retryData.candidateRunIds.map(
            (id) => id as Id<"taskRuns">
          ),
          isFallback: false,
          evaluationNote: `Summarization failed: ${message}`,
        });

        return { success: false, reason: "Summarization failed" };
      }

      if (!summary || summary.trim().length === 0) {
        await ctx.runMutation(internal.crown.workerFinalize, {
          taskId: args.taskId,
          teamId: retryData.teamId,
          userId: retryData.userId,
          winnerRunId: null,
          reason: "Summarization returned empty output",
          evaluationPrompt: retryData.evaluationPrompt,
          evaluationResponse: JSON.stringify(evaluationResponse),
          candidateRunIds: retryData.candidateRunIds.map(
            (id) => id as Id<"taskRuns">
          ),
          isFallback: false,
          evaluationNote: "Summarization returned empty output",
        });
        return { success: false, reason: "Summarization returned empty output" };
      }

      // Generate PR title and description (for consistency with normal path)
      const pullRequestTitle = buildPullRequestTitle(parsedData.prompt);
      const pullRequestDescription = buildPullRequestBody({
        summary,
        prompt: parsedData.prompt,
        agentName: winnerCandidate.agentName,
        branch: winnerCandidate.newBranch || "",
        taskId: args.taskId,
        runId: winnerCandidate.runId,
      });

      // Finalize with winner (now includes summary and PR metadata)
      await ctx.runMutation(internal.crown.workerFinalize, {
        taskId: args.taskId,
        teamId: retryData.teamId,
        userId: retryData.userId,
        winnerRunId: winnerCandidate.runId as Id<"taskRuns">,
        reason: evaluationResponse.reason,
        evaluationPrompt: retryData.evaluationPrompt,
        evaluationResponse: JSON.stringify(evaluationResponse),
        candidateRunIds: retryData.candidateRunIds.map((id) => id as Id<"taskRuns">),
        isFallback: false,
        summary,
        pullRequestTitle,
        pullRequestDescription,
      });

      // Clear retry data on success
      await ctx.runMutation(internal.tasks.clearCrownRetryData, {
        taskId: args.taskId,
      });

      console.log(`[Crown] Retry evaluation succeeded, winner: ${winnerCandidate.runId}`);
      return { success: true, winnerRunId: winnerCandidate.runId };
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error);
      console.error("[Crown] Retry evaluation failed", { taskId: args.taskId, error: message });
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: retryData.teamId,
        userId: retryData.userId,
        status: "error",
        errorMessage: message,
      });
      throw error;
    }
  },
});

// Type definitions for server-side crown evaluation
interface TaskRunData {
  _id: Id<"taskRuns">;
  agentName?: string | null;
  newBranch?: string | null;
  gitDiff?: string | null;
  status: string;
}

interface TaskData {
  _id: Id<"tasks">;
  text: string;
  teamId: string;
  userId: string;
}

interface EvaluationArgs {
  taskId: Id<"tasks">;
  teamId: string;
  userId: string;
}

/**
 * Build evaluation prompt for retry data storage (local version)
 */
function buildCrownEvaluationPromptLocal(
  prompt: string,
  candidates: Array<{
    runId?: string;
    agentName?: string;
    modelName?: string;
    gitDiff: string;
    newBranch?: string | null;
    index?: number;
  }>
): string {
  return `Task: ${prompt}
Candidates: ${JSON.stringify(
    candidates.map((c, idx) => ({
      runId: c.runId,
      agentName: c.agentName,
      modelName: c.modelName,
      gitDiff: c.gitDiff,
      newBranch: c.newBranch,
      index: c.index ?? idx,
    }))
  )}`;
}

/**
 * Server-side crown evaluation action.
 * Called by crownWorkerComplete when all task runs are finished.
 * This ensures crown evaluation happens even if the worker process terminates.
 */
export const evaluateFromServer = internalAction({
  args: {
    taskId: v.id("tasks"),
    teamId: v.string(),
    userId: v.string(),
  },
  handler: async (ctx, args): Promise<{
    success: boolean;
    alreadyEvaluated?: boolean;
    winnerRunId?: Id<"taskRuns"> | null;
    reason?: string;
  }> => {
    console.log(`[Crown] Starting server-side evaluation for task ${args.taskId}`);

    // Get task
    const task = await ctx.runQuery(internal.tasks.getByIdInternal, {
      id: args.taskId,
    }) as TaskData | null;

    if (!task) {
      console.error(`[Crown] Task ${args.taskId} not found`);
      return { success: false, reason: "Task not found" };
    }

    if (task.teamId !== args.teamId || task.userId !== args.userId) {
      console.error(`[Crown] Task ${args.taskId} unauthorized`);
      return { success: false, reason: "Unauthorized" };
    }

    // Check if evaluation already exists
    const existingEvaluation = await ctx.runQuery(
      internal.crown.getEvaluationByTaskInternal,
      {
        taskId: args.taskId,
        teamId: args.teamId,
        userId: args.userId,
      }
    ) as { winnerRunId: Id<"taskRuns"> | null } | null;

    if (existingEvaluation) {
      console.log(`[Crown] Evaluation already exists for task ${args.taskId}`);
      return {
        success: true,
        alreadyEvaluated: true,
        winnerRunId: existingEvaluation.winnerRunId,
      };
    }

    // Get all runs for this task
    const runs = await ctx.runQuery(internal.taskRuns.listByTaskAndTeamInternal, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
    }) as TaskRunData[];

    const completedRuns = runs.filter((run) => run.status === "completed");

    console.log(
      `[Crown] Task ${args.taskId} has ${completedRuns.length} completed runs out of ${runs.length} total`
    );

    if (completedRuns.length === 0) {
      console.log(`[Crown] No completed runs for task ${args.taskId}`);
      return { success: false, reason: "No completed runs" };
    }

    // Set crown status to in_progress
    await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      status: "in_progress",
      clearError: true,
    });

    try {
      if (completedRuns.length === 1) {
        // Single run - skip AI evaluation, just summarize and crown
        return await handleSingleRunCrown(ctx, task, completedRuns[0], args);
      } else {
        // Multiple runs - perform full AI evaluation
        return await handleMultiRunEvaluation(ctx, task, completedRuns, args);
      }
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error);
      console.error(`[Crown] Server-side evaluation failed for task ${args.taskId}:`, message);

      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: args.teamId,
        userId: args.userId,
        status: "error",
        errorMessage: message,
      });

      return { success: false, reason: message };
    }
  },
});

/**
 * Handle single-run crown evaluation (no AI comparison needed).
 * Just summarize the changes and crown the single winner.
 */
async function handleSingleRunCrown(
  ctx: ActionCtx,
  task: TaskData,
  run: TaskRunData,
  args: EvaluationArgs
): Promise<{ success: boolean; winnerRunId?: Id<"taskRuns">; reason?: string }> {
  console.log(`[Crown] Single run crown for task ${args.taskId}, run ${run._id}`);

  const gitDiff = run.gitDiff ?? "";
  const agentName = run.agentName ?? "unknown agent";

  // Build retry data for potential future retries
  const candidates = [
    {
      runId: run._id,
      agentName,
      modelName: agentName,
      gitDiff,
      newBranch: run.newBranch ?? null,
      index: 0,
    },
  ];

  const evaluationPrompt = buildCrownEvaluationPromptLocal(task.text, candidates);

  // Store retry data
  const retryData = JSON.stringify({
    evaluationPrompt,
    candidateRunIds: [run._id],
    teamId: args.teamId,
    userId: args.userId,
  });

  await ctx.runMutation(internal.tasks.setCrownRetryDataInternal, {
    taskId: args.taskId,
    teamId: args.teamId,
    userId: args.userId,
    retryData,
    overwrite: false,
  });

  // Perform summarization
  let summary: string | undefined;
  try {
    console.log(`[Crown] Summarizing single run ${run._id}`);
    const summaryResponse = await performCrownSummarization(task.text, gitDiff);
    summary = summaryResponse?.summary?.slice(0, 8000);
  } catch (summaryError) {
    const message =
      summaryError instanceof Error ? summaryError.message : String(summaryError);
    console.error(`[Crown] Summary failed for single run ${run._id}:`, message);

    await ctx.runMutation(internal.crown.workerFinalize, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      winnerRunId: null,
      reason: "Summarization failed",
      evaluationPrompt,
      evaluationResponse: JSON.stringify({ winner: 0, reason: "Single run" }),
      candidateRunIds: [run._id],
      isFallback: false,
      evaluationNote: `Summarization failed: ${message}`,
    });

    return { success: false, reason: "Summarization failed" };
  }

  if (!summary || summary.trim().length === 0) {
    await ctx.runMutation(internal.crown.workerFinalize, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      winnerRunId: null,
      reason: "Summarization returned empty output",
      evaluationPrompt,
      evaluationResponse: JSON.stringify({ winner: 0, reason: "Single run" }),
      candidateRunIds: [run._id],
      isFallback: false,
      evaluationNote: "Summarization returned empty output",
    });

    return { success: false, reason: "Summarization returned empty output" };
  }

  // Generate PR metadata
  const pullRequestTitle = buildPullRequestTitle(task.text);
  const pullRequestDescription = buildPullRequestBody({
    summary,
    prompt: task.text,
    agentName,
    branch: run.newBranch ?? "",
    taskId: args.taskId,
    runId: run._id,
  });

  // Finalize with the single run as winner
  await ctx.runMutation(internal.crown.workerFinalize, {
    taskId: args.taskId,
    teamId: args.teamId,
    userId: args.userId,
    winnerRunId: run._id,
    reason: "Only one model completed the task",
    summary,
    evaluationPrompt,
    evaluationResponse: JSON.stringify({
      winner: 0,
      reason: "Only one model completed the task",
    }),
    candidateRunIds: [run._id],
    isFallback: false,
    pullRequestTitle,
    pullRequestDescription,
  });

  console.log(`[Crown] Single run crown completed for task ${args.taskId}`);
  return { success: true, winnerRunId: run._id };
}

/**
 * Handle multi-run crown evaluation with AI comparison.
 */
async function handleMultiRunEvaluation(
  ctx: ActionCtx,
  task: TaskData,
  completedRuns: TaskRunData[],
  args: EvaluationArgs
): Promise<{ success: boolean; winnerRunId?: Id<"taskRuns">; reason?: string }> {
  console.log(
    `[Crown] Multi-run evaluation for task ${args.taskId} with ${completedRuns.length} candidates`
  );

  // Build candidates for evaluation
  const candidates: CrownEvaluationCandidate[] = completedRuns.map((run, idx) => ({
    runId: run._id,
    agentName: run.agentName ?? `candidate-${idx}`,
    modelName: run.agentName ?? `candidate-${idx}`,
    gitDiff: run.gitDiff ?? "",
    newBranch: run.newBranch ?? null,
    index: idx,
  }));

  const evaluationPrompt = buildCrownEvaluationPromptLocal(task.text, candidates);
  const candidateRunIds = completedRuns.map((r) => r._id);

  // Store retry data
  const retryData = JSON.stringify({
    evaluationPrompt,
    candidateRunIds,
    teamId: args.teamId,
    userId: args.userId,
  });

  await ctx.runMutation(internal.tasks.setCrownRetryDataInternal, {
    taskId: args.taskId,
    teamId: args.teamId,
    userId: args.userId,
    retryData,
    overwrite: true,
  });

  // Perform AI evaluation
  const evaluationResponse = await performCrownEvaluation(task.text, candidates);

  console.log(`[Crown] Multi-run evaluation result:`, {
    winner: evaluationResponse.winner,
    isFallback: evaluationResponse.isFallback,
  });

  if (evaluationResponse.winner === null) {
    // Evaluation failed - store error state
    await ctx.runMutation(internal.crown.workerFinalize, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      winnerRunId: null,
      reason: evaluationResponse.reason,
      evaluationPrompt,
      evaluationResponse: JSON.stringify(evaluationResponse),
      candidateRunIds,
      isFallback: true,
      evaluationNote:
        evaluationResponse.evaluationNote || "Evaluation failed",
    });

    return { success: false, reason: evaluationResponse.reason };
  }

  // Get winning candidate
  const winnerCandidate = candidates[evaluationResponse.winner];
  if (!winnerCandidate) {
    const errorMsg = `Winner index ${evaluationResponse.winner} out of bounds`;
    console.error(`[Crown] ${errorMsg}`);

    await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      status: "error",
      errorMessage: errorMsg,
    });

    return { success: false, reason: errorMsg };
  }

  // Summarize winner's changes
  let summary: string | undefined;
  try {
    console.log(`[Crown] Summarizing winner ${winnerCandidate.runId}`);
    const summaryResponse = await performCrownSummarization(
      task.text,
      winnerCandidate.gitDiff ?? ""
    );
    summary = summaryResponse?.summary?.slice(0, 8000);
  } catch (summaryError) {
    const message =
      summaryError instanceof Error ? summaryError.message : String(summaryError);
    console.error(`[Crown] Summary failed for winner:`, message);

    await ctx.runMutation(internal.crown.workerFinalize, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      winnerRunId: null,
      reason: "Summarization failed",
      evaluationPrompt,
      evaluationResponse: JSON.stringify(evaluationResponse),
      candidateRunIds,
      isFallback: false,
      evaluationNote: `Summarization failed: ${message}`,
    });

    return { success: false, reason: "Summarization failed" };
  }

  if (!summary || summary.trim().length === 0) {
    await ctx.runMutation(internal.crown.workerFinalize, {
      taskId: args.taskId,
      teamId: args.teamId,
      userId: args.userId,
      winnerRunId: null,
      reason: "Summarization returned empty output",
      evaluationPrompt,
      evaluationResponse: JSON.stringify(evaluationResponse),
      candidateRunIds,
      isFallback: false,
      evaluationNote: "Summarization returned empty output",
    });

    return { success: false, reason: "Summarization returned empty output" };
  }

  // Generate PR metadata
  const pullRequestTitle = buildPullRequestTitle(task.text);
  const pullRequestDescription = buildPullRequestBody({
    summary,
    prompt: task.text,
    agentName: winnerCandidate.agentName ?? "unknown",
    branch: winnerCandidate.newBranch ?? "",
    taskId: args.taskId,
    runId: winnerCandidate.runId ?? "",
  });

  // Finalize with winner
  await ctx.runMutation(internal.crown.workerFinalize, {
    taskId: args.taskId,
    teamId: args.teamId,
    userId: args.userId,
    winnerRunId: winnerCandidate.runId as Id<"taskRuns">,
    reason: evaluationResponse.reason,
    summary,
    evaluationPrompt,
    evaluationResponse: JSON.stringify(evaluationResponse),
    candidateRunIds,
    isFallback: false,
    pullRequestTitle,
    pullRequestDescription,
  });

  // Clear retry data on success
  await ctx.runMutation(internal.tasks.clearCrownRetryData, {
    taskId: args.taskId,
  });

  console.log(
    `[Crown] Multi-run evaluation completed, winner: ${winnerCandidate.runId}`
  );
  return { success: true, winnerRunId: winnerCandidate.runId as Id<"taskRuns"> };
}
