"use node";

import { createAnthropic } from "@ai-sdk/anthropic";
import { createGoogleGenerativeAI } from "@ai-sdk/google";
import { createOpenAI } from "@ai-sdk/openai";
import { generateObject, type LanguageModel } from "ai";
import { ConvexError, v } from "convex/values";
import {
  CrownEvaluationResponseSchema,
  CrownSummarizationResponseSchema,
  type CrownEvaluationCandidate,
  type CrownEvaluationResponse,
  type CrownSummarizationResponse,
  type CrownUnifiedResponse,
} from "@cmux/shared/convex-safe";
import {
  CLOUDFLARE_OPENAI_BASE_URL,
  CLOUDFLARE_ANTHROPIC_BASE_URL,
  CLOUDFLARE_GEMINI_BASE_URL,
} from "@cmux/shared";
import { action, internalAction } from "../_generated/server";
import { internal } from "../_generated/api";
import type { Id } from "../_generated/dataModel";

const OPENAI_CROWN_MODEL = "gpt-5-mini-2025-08-07";
const ANTHROPIC_CROWN_MODEL = "claude-sonnet-4-5-20250929";
const GEMINI_CROWN_MODEL = "gemini-3-flash-preview";

const CROWN_PROVIDERS = ["openai", "anthropic", "gemini"] as const;
type CrownProvider = (typeof CROWN_PROVIDERS)[number];

// Configuration for retry logic
const MAX_CROWN_EVALUATION_ATTEMPTS = 3;
const RETRY_BASE_DELAY_MS = 1000; // 1 second base delay, doubles each retry

/**
 * Build PR title from task prompt (inline version for Convex action)
 */
function buildPullRequestTitle(prompt: string): string {
  const base = prompt.trim() || "cmux changes";
  const title = `[Crown] ${base}`;
  return title.length > 72 ? `${title.slice(0, 69)}...` : title;
}

/**
 * Build PR body/description from task details (inline version for Convex action)
 */
function buildPullRequestBody({
  summary,
  prompt,
  agentName,
  branch,
  taskId,
  runId,
}: {
  summary?: string;
  prompt: string;
  agentName: string;
  branch: string;
  taskId: string;
  runId: string;
}): string {
  const bodySummary = summary?.trim() || "Summary not available.";
  return `## Crown Winner: ${agentName}

### Task Description
${prompt}

### Summary
${bodySummary}

### Implementation Details
- **Agent**: ${agentName}
- **Task ID**: ${taskId}
- **Run ID**: ${runId}
- **Branch**: ${branch}
- **Created**: ${new Date().toISOString()}`;
}

/**
 * Delays execution for exponential backoff
 */
function delay(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

const CrownEvaluationCandidateValidator = v.object({
  runId: v.optional(v.string()),
  agentName: v.optional(v.string()),
  modelName: v.optional(v.string()),
  gitDiff: v.string(),
  newBranch: v.optional(v.union(v.string(), v.null())),
  index: v.optional(v.number()),
});

function resolveCrownModel(): {
  provider: CrownProvider;
  model: LanguageModel;
} {
  // Note: AIGATEWAY_* accessed via process.env to avoid Convex static analysis
  const openaiKey = process.env.OPENAI_API_KEY;
  if (openaiKey) {
    const openai = createOpenAI({
      apiKey: openaiKey,
      baseURL: process.env.AIGATEWAY_OPENAI_BASE_URL || CLOUDFLARE_OPENAI_BASE_URL,
    });
    return { provider: "openai", model: openai(OPENAI_CROWN_MODEL) };
  }

  const anthropicKey = process.env.ANTHROPIC_API_KEY;
  if (anthropicKey) {
    const anthropic = createAnthropic({
      apiKey: anthropicKey,
      baseURL:
        process.env.AIGATEWAY_ANTHROPIC_BASE_URL || CLOUDFLARE_ANTHROPIC_BASE_URL,
    });
    return {
      provider: "anthropic",
      model: anthropic(ANTHROPIC_CROWN_MODEL),
    };
  }

  const geminiKey = process.env.GEMINI_API_KEY;
  if (geminiKey) {
    const google = createGoogleGenerativeAI({
      apiKey: geminiKey,
      baseURL: process.env.AIGATEWAY_GEMINI_BASE_URL || CLOUDFLARE_GEMINI_BASE_URL,
    });
    return { provider: "gemini", model: google(GEMINI_CROWN_MODEL) };
  }

  throw new ConvexError(
    "Crown evaluation is not configured (missing OpenAI, Anthropic, or Gemini API key)"
  );
}

export async function performCrownEvaluation(
  prompt: string,
  candidates: CrownEvaluationCandidate[]
): Promise<CrownEvaluationResponse> {
  const { model, provider } = resolveCrownModel();

  const normalizedCandidates = candidates.map((candidate, idx) => {
    const resolvedIndex = candidate.index ?? idx;
    return {
      index: resolvedIndex,
      runId: candidate.runId,
      agentName: candidate.agentName,
      modelName:
        candidate.modelName ??
        candidate.agentName ??
        (candidate.runId ? `run-${candidate.runId}` : undefined) ??
        `candidate-${resolvedIndex}`,
      gitDiff: candidate.gitDiff,
      newBranch: candidate.newBranch ?? null,
    };
  });

  const evaluationData = {
    prompt,
    candidates: normalizedCandidates,
  };

  const evaluationPrompt = `You are evaluating code implementations from different AI models.

Here are the candidates to evaluate:
${JSON.stringify(evaluationData, null, 2)}

NOTE: The git diffs shown contain only actual code changes. Lock files, build artifacts, and other non-essential files have been filtered out.

Analyze these implementations and select the best one based on:
1. Code quality and correctness
2. Completeness of the solution
3. Following best practices
4. Actually having meaningful code changes (if one has no changes, prefer the one with changes)

Respond with a JSON object containing:
- "winner": the index (0-based) of the best implementation
- "reason": a brief explanation of why this implementation was chosen

Example response:
{"winner": 0, "reason": "Model claude/sonnet-4 provided a more complete implementation with better error handling and cleaner code structure."}

IMPORTANT: Respond ONLY with the JSON object, no other text.`;

  // Track errors for diagnostics
  const attemptErrors: Array<{ attempt: number; error: unknown }> = [];

  // Retry loop with exponential backoff
  for (let attempt = 1; attempt <= MAX_CROWN_EVALUATION_ATTEMPTS; attempt++) {
    try {
      console.info(
        `[convex.crown] Evaluation attempt ${attempt}/${MAX_CROWN_EVALUATION_ATTEMPTS} via ${provider}`
      );

      const { object } = await generateObject({
        model,
        schema: CrownEvaluationResponseSchema,
        system:
          "You select the best implementation from structured diff inputs and explain briefly why.",
        prompt: evaluationPrompt,
        maxRetries: 2,
      });

      console.info(`[convex.crown] Evaluation completed via ${provider}`);
      const result = CrownEvaluationResponseSchema.parse(object);

      // If AI returned null winner (e.g., no code changes), default to candidate 0
      // This is different from isFallback=true which indicates AI service failure
      if (result.winner === null && normalizedCandidates.length > 0) {
        console.info(
          `[convex.crown] AI returned null winner, defaulting to candidate 0`
        );
        return {
          ...result,
          winner: 0,
          reason:
            result.reason ||
            "No meaningful code changes detected; selecting first candidate as default.",
        };
      }

      return result;
    } catch (error) {
      attemptErrors.push({ attempt, error });

      const errorMessage =
        error instanceof Error ? error.message : String(error);
      console.error(
        `[convex.crown] ${provider} evaluation attempt ${attempt}/${MAX_CROWN_EVALUATION_ATTEMPTS} failed:`,
        errorMessage
      );

      // If not the last attempt, wait with exponential backoff before retrying
      if (attempt < MAX_CROWN_EVALUATION_ATTEMPTS) {
        const backoffDelay = RETRY_BASE_DELAY_MS * Math.pow(2, attempt - 1);
        console.info(
          `[convex.crown] Retrying in ${backoffDelay}ms (attempt ${attempt + 1}/${MAX_CROWN_EVALUATION_ATTEMPTS})`
        );
        await delay(backoffDelay);
      }
    }
  }

  // All retry attempts exhausted - fall back to "no winner" state
  console.warn(
    `[convex.crown] All ${MAX_CROWN_EVALUATION_ATTEMPTS} evaluation attempts failed via ${provider}. Falling back to no-winner state.`,
    {
      provider,
      totalAttempts: MAX_CROWN_EVALUATION_ATTEMPTS,
      errors: attemptErrors.map((e) => ({
        attempt: e.attempt,
        error: e.error instanceof Error ? e.error.message : String(e.error),
      })),
    }
  );

  // Return fallback response with metadata (null winner indicates failure)
  const fallbackReason =
    "Evaluation service unavailable - no winner selected";
  return {
    winner: null,
    reason: fallbackReason,
    isFallback: true,
    evaluationNote: `Crown evaluation failed after ${MAX_CROWN_EVALUATION_ATTEMPTS} attempts (provider: ${provider}). No winner was selected.`,
  };
}

export async function performCrownSummarization(
  prompt: string,
  gitDiff: string
): Promise<CrownSummarizationResponse> {
  const { model, provider } = resolveCrownModel();

  const summarizationPrompt = `You are an expert reviewer summarizing a pull request.

GOAL
- Explain succinctly what changed and why.
- Call out areas the user should review carefully.
- Provide a quick test plan to validate the changes.

CONTEXT
- User's original request:
${prompt}
- Relevant diffs (unified):
${gitDiff || "<no code changes captured>"}

INSTRUCTIONS
- Base your summary strictly on the provided diffs and request.
- Be specific about files and functions when possible.
- Prefer clear bullet points over prose. Keep it under ~300 words.
- If there are no code changes, say so explicitly and suggest next steps.

OUTPUT FORMAT (Markdown)
## PR Review Summary
- What Changed: bullet list
- Review Focus: bullet list (risks/edge cases)
- Test Plan: bullet list of practical steps
- Follow-ups: optional bullets if applicable
`;

  try {
    const { object } = await generateObject({
      model,
      schema: CrownSummarizationResponseSchema,
      system:
        "You are an expert reviewer summarizing pull requests. Provide a clear, concise summary following the requested format.",
      prompt: summarizationPrompt,
      maxRetries: 2,
    });

    console.info(`[convex.crown] Summarization completed via ${provider}`);
    return CrownSummarizationResponseSchema.parse(object);
  } catch (error) {
    console.error(`[convex.crown] ${provider} summarization error`, error);
    throw new ConvexError("Summarization failed");
  }
}

/**
 * Performs crown evaluation AND summarization as an atomic operation.
 * If either step fails, the entire operation fails.
 * This ensures we never have a crowned winner without a summary.
 */
export async function performCrownEvaluationAndSummarization(
  prompt: string,
  candidates: CrownEvaluationCandidate[]
): Promise<CrownUnifiedResponse> {
  // Step 1: Perform evaluation
  console.info(`[convex.crown] Starting unified evaluation + summarization`);

  const evaluationResponse = await performCrownEvaluation(prompt, candidates);

  // If evaluation failed (no winner selected), return failure
  if (evaluationResponse.winner === null) {
    console.warn(`[convex.crown] Unified flow failed at evaluation step`);
    return {
      success: false,
      winner: null,
      reason: evaluationResponse.reason,
      isFallback: evaluationResponse.isFallback,
      errorMessage: evaluationResponse.evaluationNote || "Evaluation failed to select a winner",
      failedStep: "evaluation",
    };
  }

  // Get the winner candidate for summarization
  const winnerCandidate = candidates[evaluationResponse.winner];
  if (!winnerCandidate) {
    console.error(`[convex.crown] Winner index ${evaluationResponse.winner} out of bounds`);
    return {
      success: false,
      winner: null,
      reason: evaluationResponse.reason,
      errorMessage: `Winner index ${evaluationResponse.winner} is out of bounds`,
      failedStep: "evaluation",
    };
  }

  // Step 2: Perform summarization
  try {
    console.info(`[convex.crown] Evaluation succeeded, starting summarization for winner`);
    const summarizationResponse = await performCrownSummarization(prompt, winnerCandidate.gitDiff);

    // Success! Both steps completed
    console.info(`[convex.crown] Unified evaluation + summarization completed successfully`);
    return {
      success: true,
      winner: evaluationResponse.winner,
      reason: evaluationResponse.reason,
      summary: summarizationResponse.summary,
      isFallback: evaluationResponse.isFallback,
    };
  } catch (error) {
    // Summarization failed - treat the entire operation as failed
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error(`[convex.crown] Unified flow failed at summarization step:`, errorMessage);

    return {
      success: false,
      winner: evaluationResponse.winner, // Include winner for retry context
      reason: evaluationResponse.reason,
      isFallback: true,
      errorMessage: `Summarization failed: ${errorMessage}`,
      failedStep: "summarization",
    };
  }
}

export const evaluate = action({
  args: {
    prompt: v.string(),
    candidates: v.array(CrownEvaluationCandidateValidator),
    teamSlugOrId: v.string(),
  },
  handler: async (_ctx, args) => {
    return performCrownEvaluation(args.prompt, args.candidates);
  },
});

export const summarize = action({
  args: {
    prompt: v.string(),
    gitDiff: v.string(),
    teamSlugOrId: v.string(),
  },
  handler: async (_ctx, args) => {
    return performCrownSummarization(args.prompt, args.gitDiff);
  },
});

/**
 * Unified action that performs evaluation + summarization atomically.
 * Used by the worker to ensure both steps succeed or both fail.
 */
export const evaluateAndSummarize = action({
  args: {
    prompt: v.string(),
    candidates: v.array(CrownEvaluationCandidateValidator),
    teamSlugOrId: v.string(),
  },
  handler: async (_ctx, args) => {
    return performCrownEvaluationAndSummarization(args.prompt, args.candidates);
  },
});

/**
 * Schema for parsing the stored retry data
 */
interface CrownRetryData {
  evaluationPrompt: string;
  candidateRunIds: string[];
  teamId: string;
  userId: string;
}

/**
 * Schema for parsing evaluation prompt to extract candidates
 * Format: "Task: <prompt>\nCandidates: <JSON array>"
 */
interface ParsedEvaluationData {
  prompt: string;
  candidates: Array<{
    runId: string;
    agentName: string;
    modelName?: string;
    gitDiff: string;
    newBranch?: string | null;
    index: number;
  }>;
}

function parseEvaluationPrompt(evaluationPrompt: string): ParsedEvaluationData | null {
  try {
    // Format is "Task: <prompt>\nCandidates: <JSON>"
    const taskMatch = evaluationPrompt.match(/^Task:\s*(.+?)\nCandidates:\s*/s);
    if (!taskMatch) {
      console.error("[Crown] Failed to parse evaluation prompt: no Task match");
      return null;
    }

    const prompt = taskMatch[1].trim();
    const candidatesJson = evaluationPrompt.slice(taskMatch[0].length);

    const candidates = JSON.parse(candidatesJson);
    if (!Array.isArray(candidates)) {
      console.error("[Crown] Failed to parse evaluation prompt: candidates not array");
      return null;
    }

    return { prompt, candidates };
  } catch (error) {
    console.error("[Crown] Failed to parse evaluation prompt:", error);
    return null;
  }
}

/**
 * Internal action to retry a failed crown evaluation.
 * Called after retryCrownEvaluation mutation resets the status.
 * Uses the unified flow - both evaluation AND summarization must succeed.
 */
export const retryEvaluation = internalAction({
  args: {
    taskId: v.id("tasks"),
  },
  handler: async (ctx, args) => {
    console.log(`[Crown] Starting retry evaluation for task ${args.taskId}`);

    // Get task with retry data
    const task = await ctx.runQuery(internal.tasks.getByIdInternal, {
      id: args.taskId,
    });

    if (!task) {
      throw new Error("Task not found");
    }

    if (!task.crownEvaluationRetryData) {
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: task.teamId,
        userId: task.userId,
        status: "error",
        errorMessage: "No retry data available for this task",
      });
      throw new Error("No retry data available for this task");
    }

    // Parse retry data
    let retryData: CrownRetryData;
    try {
      retryData = JSON.parse(task.crownEvaluationRetryData);
    } catch (error) {
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: task.teamId,
        userId: task.userId,
        status: "error",
        errorMessage: "Invalid retry data format",
      });
      throw new Error("Invalid retry data format");
    }

    // Parse the evaluation prompt to extract candidates
    const parsedData = parseEvaluationPrompt(retryData.evaluationPrompt);
    if (!parsedData) {
      // Mark as error if we can't parse the data
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: retryData.teamId,
        userId: retryData.userId,
        status: "error",
        errorMessage: "Failed to parse stored evaluation data",
      });
      throw new Error("Failed to parse stored evaluation data");
    }

    // Mark as in_progress
    await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
      taskId: args.taskId,
      teamId: retryData.teamId,
      userId: retryData.userId,
      status: "in_progress",
      clearError: true,
    });

    // Prepare candidates for evaluation
    const evaluationCandidates: CrownEvaluationCandidate[] = parsedData.candidates.map(
      (candidate, idx) => ({
        runId: candidate.runId,
        agentName: candidate.agentName,
        modelName: candidate.modelName || candidate.agentName,
        gitDiff: candidate.gitDiff,
        newBranch: candidate.newBranch,
        index: candidate.index ?? idx,
      })
    );

    console.log(`[Crown] Retrying with unified evaluation + summarization for ${evaluationCandidates.length} candidates`);

    try {
      // Perform unified evaluation + summarization (atomic operation)
      const unifiedResponse = await performCrownEvaluationAndSummarization(
        parsedData.prompt,
        evaluationCandidates
      );

      console.log(`[Crown] Retry unified result:`, {
        success: unifiedResponse.success,
        winner: unifiedResponse.winner,
        failedStep: unifiedResponse.failedStep,
        isFallback: unifiedResponse.isFallback,
      });

      // Handle failure (either evaluation or summarization failed)
      if (!unifiedResponse.success) {
        const errorNote = unifiedResponse.errorMessage ||
          (unifiedResponse.failedStep === "summarization"
            ? "Summarization failed after successful evaluation"
            : "Evaluation failed to select a winner");

        await ctx.runMutation(internal.crown.workerFinalize, {
          taskId: args.taskId,
          teamId: retryData.teamId,
          userId: retryData.userId,
          winnerRunId: null,
          reason: unifiedResponse.reason,
          evaluationPrompt: retryData.evaluationPrompt,
          evaluationResponse: JSON.stringify(unifiedResponse),
          candidateRunIds: retryData.candidateRunIds.map((id) => id as Id<"taskRuns">),
          isFallback: true,
          evaluationNote: errorNote,
        });

        return { success: false, reason: unifiedResponse.reason, failedStep: unifiedResponse.failedStep };
      }

      // Success! Both evaluation and summarization completed
      const winnerCandidate = parsedData.candidates[unifiedResponse.winner!];
      if (!winnerCandidate) {
        throw new Error(`Winner index ${unifiedResponse.winner} out of bounds`);
      }

      const summary = unifiedResponse.summary?.slice(0, 8000);

      // Generate PR title and description
      const pullRequestTitle = buildPullRequestTitle(parsedData.prompt);
      const pullRequestDescription = buildPullRequestBody({
        summary,
        prompt: parsedData.prompt,
        agentName: winnerCandidate.agentName,
        branch: winnerCandidate.newBranch || "",
        taskId: args.taskId,
        runId: winnerCandidate.runId,
      });

      // Finalize with winner (includes summary and PR metadata)
      await ctx.runMutation(internal.crown.workerFinalize, {
        taskId: args.taskId,
        teamId: retryData.teamId,
        userId: retryData.userId,
        winnerRunId: winnerCandidate.runId as Id<"taskRuns">,
        reason: unifiedResponse.reason,
        evaluationPrompt: retryData.evaluationPrompt,
        evaluationResponse: JSON.stringify(unifiedResponse),
        candidateRunIds: retryData.candidateRunIds.map((id) => id as Id<"taskRuns">),
        isFallback: false,
        summary,
        pullRequestTitle,
        pullRequestDescription,
      });

      // Clear retry data on success
      await ctx.runMutation(internal.tasks.clearCrownRetryData, {
        taskId: args.taskId,
      });

      console.log(`[Crown] Retry succeeded with unified flow, winner: ${winnerCandidate.runId}`);
      return { success: true, winnerRunId: winnerCandidate.runId };
    } catch (error) {
      const message = error instanceof Error ? error.message : String(error);
      console.error("[Crown] Retry evaluation failed", { taskId: args.taskId, error: message });
      await ctx.runMutation(internal.tasks.setCrownEvaluationStatusInternal, {
        taskId: args.taskId,
        teamId: retryData.teamId,
        userId: retryData.userId,
        status: "error",
        errorMessage: message,
      });
      throw error;
    }
  },
});
